<!DOCTYPE html>
<html lang="en-us" class="h-100">
  <head>
    <title>
      Not every published model is a valid benchmark ‚Äì dr-marco ‚Äì dr-marco's blog
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <!-- The page supports both dark and light color schemes, and the page author prefers / default is light. -->
    <meta name="color-scheme" content="dark light" />
    <!-- Stylesheet that includes Bootstrap dark mode & custom stylesheet -->
    <link
      rel="stylesheet"
      href="/it/assets/css/styles.css"
    />
    <link rel="icon" href="/it/assets/images/favicon.ico" />
    <!-- Meta Theme Color is also supported on Safari and Chrome -->
    <meta
      name="theme-color"
      content="#eeeeee"
      media="(prefers-color-scheme: dark)"
    />
    <meta
      name="theme-color"
      content="#111111"
      media="(prefers-color-scheme: light)"
    />
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Not every published model is a valid benchmark | dr-marco‚Äôs blog</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Not every published model is a valid benchmark" />
<meta name="author" content="dr-marco" />
<meta property="og:locale" content="en" />
<meta name="description" content="Training a model with deep leaning techniques could be very complex if you don‚Äôt know how to set up properly hyperparameters. To avoid headaches you can tune them and find the subset of values that will optimize the performances of your model. Or if you are short on time you could do a little research and look if somebody already did the tuning part. You can register the hyperparameters configuration and also take others results as benchmark for your model. The purpose of doing that is to get a starting point for your tuning and a benchmark to improve if possible. This is in the end what I wanted to do in one of the last projects that I did at university." />
<meta property="og:description" content="Training a model with deep leaning techniques could be very complex if you don‚Äôt know how to set up properly hyperparameters. To avoid headaches you can tune them and find the subset of values that will optimize the performances of your model. Or if you are short on time you could do a little research and look if somebody already did the tuning part. You can register the hyperparameters configuration and also take others results as benchmark for your model. The purpose of doing that is to get a starting point for your tuning and a benchmark to improve if possible. This is in the end what I wanted to do in one of the last projects that I did at university." />
<link rel="canonical" href="http://localhost:4000/it/2025/01/16/plastic-waste-detection" />
<meta property="og:url" content="http://localhost:4000/2025/01/16/plastic-waste-detection" />
<meta property="og:site_name" content="dr-marco‚Äôs blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-01-07T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Not every published model is a valid benchmark" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"dr-marco"},"dateModified":"2025-01-07T00:00:00+01:00","datePublished":"2025-01-07T00:00:00+01:00","description":"Training a model with deep leaning techniques could be very complex if you don‚Äôt know how to set up properly hyperparameters. To avoid headaches you can tune them and find the subset of values that will optimize the performances of your model. Or if you are short on time you could do a little research and look if somebody already did the tuning part. You can register the hyperparameters configuration and also take others results as benchmark for your model. The purpose of doing that is to get a starting point for your tuning and a benchmark to improve if possible. This is in the end what I wanted to do in one of the last projects that I did at university.","headline":"Not every published model is a valid benchmark","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2025/01/16/plastic-waste-detection"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/k2so_01.png"},"name":"dr-marco"},"url":"http://localhost:4000/2025/01/16/plastic-waste-detection"}</script>
<!-- End Jekyll SEO tag -->
 <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/it/feed.xml" title="dr-marco&apos;s blog" /></head>
  <body class="d-flex flex-column h-100">
    <div class="container">
      <nav class="navbar navbar-expand-lg navbar-light">
  <div class="container-fluid">
    <a class="navbar-brand p-0" href="/it/">
      
      <img src="/assets/images/k2so_01_small.png" alt="light avatar" width="30"
        class="d-inline-block align-top m-0 light-avatar">
      <img src="/assets/images/k2so_02_small.png" alt="dark avatar" width="30"
        class="d-inline-block align-top m-0 dark-avatar">
      
      dr-marco</a>
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
      aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarNav">
      <ul class="navbar-nav me-auto">
        
        <li class="nav-item">
          <a href="/it/" 
            class="nav-link" >Home</a>
        </li>
        
        <li class="nav-item">
          <a href="/it/books/" 
            class="nav-link" >Books</a>
        </li>
        
        <li class="nav-item">
          <a href="/it/about/" 
            class="nav-link" >About</a>
        </li>
        
        <li class="nav-item">
          <a href="/it/portfolio/" 
            class="nav-link" >Portfolio</a>
        </li>
        
      </ul>
    </div>
    <ul class="navbar-nav me-auto">
      
        
        
          
            <li class="nav-item">
              <a href=" /2025/01/16/plastic-waste-detection">üá¨üáß</a>
            </li>
          
        
      
        
        
üáÆüáπ
        
      
    </ul>
  </div>
</nav>
      <div class="body"><div class="post">
  <h1>Not every published model is a valid benchmark</h1>
  <span class="fs-4 text-muted">January 07, 2025</span>
  <div class="mt-4"><p>Training a model with deep leaning techniques could be very complex if you don‚Äôt know how to set up properly hyperparameters. 
To avoid headaches you can tune them and find the subset of values that will optimize the performances of your model.
Or if you are short on time you could do a little research and look if somebody already did the tuning part.
You can register the hyperparameters configuration and also take others results as benchmark for your model.
The purpose of doing that is to get a starting point for your tuning and a benchmark to improve if possible.
This is in the end what I wanted to do in one of the last projects that I did at university.</p>

<h3 id="my-assignment">My assignment</h3>

<p>My assignment was to train a model that is able to detect plastic waste in water surfaces like rivers and lakes. 
I worked with a friend of mine and both decided to use the YOLOv8 architecture and train it with a dataset that was available during our work.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>
We did some experiment without any particular result in performance so we have looked around searching for academic papers or some 
post on the internet that was somehow relevant.
We have bumped mostly in detection with dataset containing satellite images except one study published in early 2024<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> that
was related with the same our dataset and model, lucky us. 
So reading the paper, we found out that we could reach good performance with a specific configuration. Nice!</p>

<p>Let‚Äôs try if we can replicate those results and‚Ä¶ nope, still bad output with high evaluation loss.
Maybe we did the training execution with some hyperparameter or other details different. Let‚Äôs try again but with 
the very same values as hyperparameters. Nothing, performances don‚Äôt want to improve.</p>

<h3 id="wtf">WTF?!</h3>

<p>It seems reasonable to look for bugs or misconfigurations by our fault (after all I‚Äôm still a master student) but when no solution
is able to give better results some doubts come out.</p>

<p>Are we sure that the performances shown in the paper are legit? Is there any kind of catch in it?</p>

<p>So, to answer that question we have to do only one thing: find a method to replicate academic results even if it means to
make stupid assumptions on dataset, model or techniques. And we did it, we have made very poor configurations until we found out 
that the problem with the academic study wasn‚Äôt in the hyperparameters values but with the use of the dataset.</p>

<p>Trivia: what happens if you train a model with a train set, as in a dataset used for the training phase, and then you use the very
same train set also as val set, as in the dataset which purpose is to validate the model? Spoiler: amazing results shown but same shitty model.</p>

<h3 id="diving-into-details">Diving into details</h3>

<p>The dataset used <sup id="fnref:1:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> contains 3 different subsets, <code class="language-plaintext highlighter-rouge">train</code>, <code class="language-plaintext highlighter-rouge">val</code> and <code class="language-plaintext highlighter-rouge">test</code>, each designed for his specific purpose as mentioned before in the sarcastic question (the test set is used to test the model once the training phase is completed).
Inside each subset there are quite a lot different images labeled with 4 different kind of classes using the YOLO specification (see the <a href="https://docs.ultralytics.com/datasets/detect/">YOLO documentation</a> to learn more about it).</p>

<p>In order to evaluate the goodness of the trained model many metrics are used like precision, recall and loss functions. 
In particular for object detection models the main loss functions are the <code class="language-plaintext highlighter-rouge">box_loss</code> and <code class="language-plaintext highlighter-rouge">cls_loss</code>, the latter tell us the misclassification rate and the former the error during the drawing of the box over the detected objects on the image.
But one of the best metric that can summarize how good the detection model really was is the <code class="language-plaintext highlighter-rouge">mAP50</code>: mean average precision over all classes used.
Again, for more details about the metrics that can be used in object detection models check the documentation <a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/">here</a>.</p>

<p>For simplicity here I will comment only the mAP50: this percentage metric is better and better when it is higher and higher until 100%.</p>

<p>With our tests we were able to reach an overall mAP50 of 39.1%. It wasn‚Äôt very good since the model was good with a specific class, <code class="language-plaintext highlighter-rouge">PLASTIC_BOTTLE</code> and bad with the others. And as you can see in the following graphs there is no margine of improvements because the model with extra training could overfit.</p>

<p><img src="/assets/images/plastic_detection/results_1.png" alt="Loss functions and metrics of our trained model" />
<em>Loss functions and metrics of our trained model</em></p>

<p>As you can see these metrics weren‚Äôt as good as those in the paper. The authors claimed their model could reach 68.8%</p>

<p><img src="/assets/images/plastic_detection/results_2.png" alt="Loss functions and metrics of the paper model" /> 
<em>Loss functions and metrics of the paper model (sorry for bad image resolution)</em></p>

<p><img src="/assets/images/plastic_detection/results_3.png" alt="Loss functions and metrics matching paper model" width="1440px" />
<em>Quite the same behavior with the metrics</em></p>

<h3 id="ok-why-is-it-bad">Ok, why is it bad?</h3>

<p>as you can see
overfitting
memorizing the dataset and not learning the detection function</p>

<p>Using the same subset of data for both training and validation in a machine learning model is problematic because it leads to overfitting 
and an inaccurate assessment of model performance. The model essentially ‚Äúmemorizes‚Äù the training data, resulting in artificially high accuracy 
during validation. This provides no indication of how the model will perform on unseen data, undermining its generalizability and real-world 
applicability. Proper validation requires a separate, unseen dataset to evaluate the model‚Äôs ability to generalize beyond the training data.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>Dataset used: <code class="language-plaintext highlighter-rouge">plastic_in_river</code>. Unfortunately the dataset is not available anymore. <a href="http://web.archive.org/web/20240821170803/https://huggingface.co/datasets/kili-technology/plastic_in_river">Here</a> the wayback machine archive linking dataset page from huggingface in August 2024.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:2">
      <p><em>Plastic Waste on Water Surfaces Detection Using Convolutional Neural Networks</em>: <a href="https://ceur-ws.org/Vol-3668/paper13.pdf">Here</a> you can read the paper¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
</div>
</div>
</div>
    </div>
    <footer class="footer mt-auto py-3">
  <div class="container footer-container">
<a class="footer-icon-links" href="mailto:deramundo.marco@gmail.com">
  <svg
    xmlns="http://www.w3.org/2000/svg"
    width="16"
    height="16"
    fill="currentColor"
    class="bi bi-envelope-fill footer-icons"
    viewBox="0 0 16 16"
  >
    <path
      d="M.05 3.555A2 2 0 0 1 2 2h12a2 2 0 0 1 1.95 1.555L8 8.414.05 3.555zM0 4.697v7.104l5.803-3.558L0 4.697zM6.761 8.83l-6.57 4.027A2 2 0 0 0 2 14h12a2 2 0 0 0 1.808-1.144l-6.57-4.027L8 9.586l-1.239-.757zm3.436-.586L16 11.801V4.697l-5.803 3.546z"
    />
  </svg>
</a>
 
<a
  class="footer-icon-links"
  href="https://instagram.com/marco_dera"
  ><svg
    xmlns="http://www.w3.org/2000/svg"
    width="16"
    height="16"
    fill="currentColor"
    class="bi bi-instagram footer-icons"
    viewBox="0 0 16 16"
  >
    <path
      d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.917 3.917 0 0 0-1.417.923A3.927 3.927 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.916 3.916 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.926 3.926 0 0 0-.923-1.417A3.911 3.911 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0h.003zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599.28.28.453.546.598.92.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.47 2.47 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.478 2.478 0 0 1-.92-.598 2.48 2.48 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233 0-2.136.008-2.388.046-3.231.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92.28-.28.546-.453.92-.598.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045v.002zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92zm-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217zm0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334z"
    /></svg>
</a>
 
<a
  class="footer-icon-links"
  href="https://www.twitter.com/DeraMarco"
  ><svg
    xmlns="http://www.w3.org/2000/svg"
    width="16"
    height="16"
    fill="currentColor"
    class="bi bi-twitter footer-icons"
    viewBox="0 0 16 16"
  >
    <path
      d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334 0-.14 0-.282-.006-.422A6.685 6.685 0 0 0 16 3.542a6.658 6.658 0 0 1-1.889.518 3.301 3.301 0 0 0 1.447-1.817 6.533 6.533 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.325 9.325 0 0 1-6.767-3.429 3.289 3.289 0 0 0 1.018 4.382A3.323 3.323 0 0 1 .64 6.575v.045a3.288 3.288 0 0 0 2.632 3.218 3.203 3.203 0 0 1-.865.115 3.23 3.23 0 0 1-.614-.057 3.283 3.283 0 0 0 3.067 2.277A6.588 6.588 0 0 1 .78 13.58a6.32 6.32 0 0 1-.78-.045A9.344 9.344 0 0 0 5.026 15z"
    /></svg>
</a>
  
<a
  class="footer-icon-links"
  href="https://github.com/dr-marco/"
  ><svg
    xmlns="http://www.w3.org/2000/svg"
    width="16"
    height="16"
    fill="currentColor"
    class="bi bi-github footer-icons"
    viewBox="0 0 16 16"
  >
    <path
      d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"
    /></svg>
</a>
 
<a
  class="footer-icon-links"
  href="https://www.linkedin.com/in/marco-de-ramundo"
  ><svg
    xmlns="http://www.w3.org/2000/svg"
    width="16"
    height="16"
    fill="currentColor"
    class="bi bi-linkedin footer-icons"
    viewBox="0 0 16 16"
  >
    <path
      d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z"
    /></svg>
</a>
  
<a
  class="footer-icon-links"
  href="https://infosec.exchange/@dera"
  ><svg
    xmlns="http://www.w3.org/2000/svg"
    width="16"
    height="16"
    fill="currentColor"
    class="bi bi-mastodon footer-icons"
    viewBox="0 0 16 16"
  >
    <path d="M11.19 12.195c2.016-.24 3.77-1.475 3.99-2.603.348-1.778.32-4.339.32-4.339 0-3.47-2.286-4.488-2.286-4.488C12.062.238 10.083.017 8.027 0h-.05C5.92.017 3.942.238 2.79.765c0 0-2.285 1.017-2.285 4.488l-.002.662c-.004.64-.007 1.35.011 2.091.083 3.394.626 6.74 3.78 7.57 1.454.383 2.703.463 3.709.408 1.823-.1 2.847-.647 2.847-.647l-.06-1.317s-1.303.41-2.767.36c-1.45-.05-2.98-.156-3.215-1.928a4 4 0 0 1-.033-.496s1.424.346 3.228.428c1.103.05 2.137-.064 3.188-.189zm1.613-2.47H11.13v-4.08c0-.859-.364-1.295-1.091-1.295-.804 0-1.207.517-1.207 1.541v2.233H7.168V5.89c0-1.024-.403-1.541-1.207-1.541-.727 0-1.091.436-1.091 1.296v4.079H3.197V5.522q0-1.288.66-2.046c.456-.505 1.052-.764 1.793-.764.856 0 1.504.328 1.933.983L8 4.39l.417-.695c.429-.655 1.077-.983 1.934-.983.74 0 1.336.259 1.791.764q.662.757.661 2.046z"/>
  </svg>
</a>

</div>
</footer>

    <!-- JS for Bootstrap -->
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
      crossorigin="anonymous"
    ></script>
  </body>
</html>
