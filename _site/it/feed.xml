<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="deramundo.com/it/feed.xml" rel="self" type="application/atom+xml" /><link href="deramundo.com/it/" rel="alternate" type="text/html" /><updated>2025-10-24T00:17:23+02:00</updated><id>deramundo.com/feed.xml</id><title type="html">dr-marco’s blog</title><subtitle>dr-marco&apos;s blog</subtitle><author><name>dr-marco</name></author><entry xml:lang="it"><title type="html">Non tutti i modelli pubblicati sono dei benchmark validi</title><link href="deramundo.com/it/2025/10/15/plastic-waste-detection" rel="alternate" type="text/html" title="Non tutti i modelli pubblicati sono dei benchmark validi" /><published>2025-10-15T00:00:00+02:00</published><updated>2025-10-15T00:00:00+02:00</updated><id>deramundo.com/2025/10/15/plastic-waste-detection-it</id><content type="html" xml:base="deramundo.com/2025/10/15/plastic-waste-detection"><![CDATA[<p>Allenare un modello con tecniche di deep learning può essere una vera rottura se 
non sai come impostare correttamente gli iperparametri.
Per evitare inutili mal di testa, puoi fare il tuning e trovare quel sottoinsieme di valori che ottimizzerà le performance del tuo modello.
Oppure, se hai poco tempo, puoi fare una piccola ricerca e vedere se qualcuno ha 
già addestrato il modello.
Puoi recuperare la configurazione degli iperparametri e usare i loro risultati 
come benchmark per il tuo modello.
Il senso di fare ciò è avere un punto di partenza per il tuo tuning e un 
benchmark da migliorare, se possibile.
Questo è praticamente quello che volevo fare in uno degli ultimi progetti che ho 
svolto in università.</p>

<h3 id="il-mio-compito">Il mio compito</h3>

<p>Il mio compito era allenare un modello in grado di rilevare rifiuti di plastica 
sulle superfici d’acqua come fiumi e laghi.
Ho lavorato con un mio amico ed entrambi abbiamo deciso di usare l’architettura 
YOLOv8 e allenarla con un dataset che era disponibile per il nostro progetto.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>
Abbiamo fatto alcuni esperimenti senza ottenere alcun risultato degno di nota in 
termini di performance, così ci siamo guardati in giro cercando articoli 
accademici o qualche
post su internet che potesse risultare rilevante.
Siamo incappati principalmente in paper sulla detection con dataset contenenti 
immagini satellitari… tranne uno studio pubblicato all’inizio del 2024<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> che
era legato esattamente al nostro stesso dataset e modello. Che fortuna.
Così, leggendo il paper, abbiamo scoperto che potevamo raggiungere buone 
performance con una configurazione specifica. Fantastico!</p>

<p>Proviamo a replicare quei risultati e… nope, ancora risultati scadenti con una 
loss di validazione alta.
Forse avevamo eseguito il training con qualche iperparametro o dettaglio diversi. 
Proviamo di nuovo, ma con
gli esatti stessi valori di iperparametri. Ancora niente. Le performance non 
volevano migliorare.</p>

<h3 id="wtf">WTF?!</h3>

<p>Sembra ragionevole cercare bug o errori di configurazione da parte nostra 
(dopotutto, ero ancora uno studente magistrale), ma quando <em>niente</em> di ciò che 
provi
è in grado di dare risultati migliori, inizi ad avere dei dubbi.</p>

<p>Siamo sicuri che le performance mostrate nel paper siano legittime? Non è che 
c’è qualche dettaglio che non torna?</p>

<p>Quindi dovevamo fare solo una cosa: trovare un metodo per replicare i loro 
risultati, anche a costo
di fare supposizioni stupide sul dataset, sul modello o sulle tecniche utilizzate. 
E l’abbiamo fatto. Abbiamo continuato ad addestrare modelli usando 
configurazioni “sbagliate” finché non abbiamo scoperto
che il problema dello studio accademico non era affatto negli iperparametri, ma 
in come avevano usato il dataset.</p>

<p>Domanda: cosa succede se alleni un modello con un <code class="language-plaintext highlighter-rouge">train</code> set (cioè, un dataset 
usato per la fase di addestramento) e poi usi lo
<em>stesso identico</em> <code class="language-plaintext highlighter-rouge">train</code> set anche come <code class="language-plaintext highlighter-rouge">val</code> set (cioè, il dataset il cui scopo 
è validare il modello)? Spoiler: ottieni risultati che sembrano fantastici, ma 
con lo <em>stesso modello di merda</em>.</p>

<h3 id="entriamo-nei-dettagli">Entriamo nei dettagli</h3>

<p>Il dataset usato <sup id="fnref:1:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> contiene 3 sottoinsiemi diversi, <code class="language-plaintext highlighter-rouge">train</code>, <code class="language-plaintext highlighter-rouge">val</code> e <code class="language-plaintext highlighter-rouge">test</code>, 
ognuno progettato per il suo scopo specifico, come ho detto prima in quella 
domanda sarcastica (il <code class="language-plaintext highlighter-rouge">test</code> set è usato per testare il modello una volta 
completata la fase di addestramento).
Dentro ogni sottoinsieme, ci sono un sacco di immagini diverse etichettate con 4 
diversi tipi di classi usando la specifica YOLO (dai un’occhiata alla <a href="https://docs.ultralytics.com/datasets/detect/">documentazione di YOLO</a> per saperne di più).</p>

<p>Per valutare la bontà del modello addestrato, si usano molte metriche, come 
precision, recall e funzioni di loss.
In particolare, per i modelli di object detection, le principali funzioni di 
loss sono la <code class="language-plaintext highlighter-rouge">box_loss</code> e la <code class="language-plaintext highlighter-rouge">cls_loss</code>. La <code class="language-plaintext highlighter-rouge">cls_loss</code> (loss di classificazione) 
ci dice il tasso di errata classificazione, e la <code class="language-plaintext highlighter-rouge">box_loss</code> ci dice l’errore 
durante il disegno del riquadro (box) sugli oggetti rilevati.
Ma una delle migliori metriche che riassume quanto è realmente buono il modello 
di detection è la <code class="language-plaintext highlighter-rouge">mAP50</code>: mean average precision su tutte le classi usate.
Di nuovo, per maggiori dettagli sulle metriche che si possono usare nei modelli 
di object detection, controlla la documentazione <a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/">qui</a>.</p>

<p>Per semplicità, mi concentrerò solo sulla <code class="language-plaintext highlighter-rouge">mAP50</code>: questa metrica percentuale 
significa semplicemente “più alto è, meglio è”, dove 100% è la perfezione.</p>

<p>Con i nostri test effettivi (usando il <code class="language-plaintext highlighter-rouge">val</code> set corretto), siamo riusciti a 
raggiungere solo un <code class="language-plaintext highlighter-rouge">mAP50</code> complessivo del 39.1%. Non era un granché, visto che 
il modello era performante con una classe specifica, <code class="language-plaintext highlighter-rouge">PLASTIC_BOTTLE</code>, e scarso 
con le altre. E come potete vedere nei grafici seguenti, non c’è margine di 
miglioramento perché il modello andrebbe solo in overfitting con ulteriore training.</p>

<p><img src="/assets/images/plastic_detection/results_1.png" alt="Loss functions and metrics of our trained model" />
<em>Funzioni di loss e metriche del nostro modello allenato (quello legittimo)</em></p>

<p>Come potete vedere, queste metriche non erano neanche lontanamente buone come 
quelle del paper. Gli autori sostenevano che il loro modello potesse raggiungere 
il 68.8%!</p>

<p><img src="/assets/images/plastic_detection/results_2.png" alt="Loss functions and metrics of the paper model" /> 
<em>Funzioni di loss e metriche del modello del paper (scusate per la bassa risoluzione dell’immagine)</em></p>

<p><img src="/assets/images/plastic_detection/results_3.png" alt="Loss functions and metrics matching paper model" width="1440px" />
<strong><em>E guardate un po’! I nostri risultati quando copiamo il loro errore. Praticamente lo stesso comportamento.</em></strong></p>

<h3 id="ok-perché-è-così-grave">Ok, perché è così grave?</h3>

<p>Quindi, perché questa cosa è così grave?
Come potete vedere dai nostri risultati “falsati” (la terza immagine), le 
metriche sembrano fantastiche. La validation loss crolla e il mAP50 schizza al 
62.1%, praticamente proprio come sosteneva il paper.
Ma il modello non sta imparando nulla. Sta memorizzando.</p>

<p>Pensatela così: è come dare a uno studente un test di pratica da 100 domande e 
poi dargli come esame finale le stesse identiche 100 domande. Lo studente 
prenderebbe il massimo dei voti, giusto? Ma ha davvero imparato la materia? 
Neanche per idea. Ha solo imparato a memoria le risposte.</p>

<p>È esattamente quello che sta succedendo qui. Il <code class="language-plaintext highlighter-rouge">val</code> set dovrebbe essere il 
“vero test” per controllare se il modello è in grado di generalizzare, cioè se 
riesce a trovare la plastica in immagini nuove che non ha mai visto prima.
Usando il <code class="language-plaintext highlighter-rouge">train</code> set come <code class="language-plaintext highlighter-rouge">val</code> set, il modello impara semplicemente a memoria 
i dati di training. Non sta imparando a rilevare le caratteristiche di una 
bottiglia di plastica; sta imparando “nell’<em>immagine_123.jpg</em>, c’è un riquadro 
alle coordinate [x, y, w, h]”. Questa è la definizione classica di overfitting, 
ed è un errore colossale nel machine learning.
Il modello sembra un genio sulla carta, ma è completamente inutile nel mondo 
reale. Nell’istante in cui gli daresti una foto nuova dal <code class="language-plaintext highlighter-rouge">test</code> set, 
fallirebbe miseramente.</p>

<p>Quindi, la morale della favola? Siate sempre scettici. Solo perché qualcosa è 
“pubblicato” non significa che sia valido e corretto. Cercate sempre di 
replicare i risultati, e se qualcosa puzza (o è troppo bello per essere vero), 
probabilmente lo è. Fidatevi del vostro <code class="language-plaintext highlighter-rouge">val</code> set… purché sia quello giusto!</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">

      <p>Dataset usato: <code class="language-plaintext highlighter-rouge">plastic_in_river</code>. Sfortunatamente il dataset non è più disponibile. <a href="http://web.archive.org/web/20240821170803/https://huggingface.co/datasets/kili-technology/plastic_in_river">Qui</a> l’archivio della Wayback Machine che linka la pagina del dataset su Hugging Face ad agosto 2024. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:2">

      <p><em>Plastic Waste on Water Surfaces Detection Using Convolutional Neural Networks</em>: <a href="https://ceur-ws.org/Vol-3668/paper13.pdf">Qui</a> potete leggere il paper <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>dr-marco</name></author><summary type="html"><![CDATA[Allenare un modello con tecniche di deep learning può essere una vera rottura se non sai come impostare correttamente gli iperparametri. Per evitare inutili mal di testa, puoi fare il tuning e trovare quel sottoinsieme di valori che ottimizzerà le performance del tuo modello. Oppure, se hai poco tempo, puoi fare una piccola ricerca e vedere se qualcuno ha già addestrato il modello. Puoi recuperare la configurazione degli iperparametri e usare i loro risultati come benchmark per il tuo modello. Il senso di fare ciò è avere un punto di partenza per il tuo tuning e un benchmark da migliorare, se possibile. Questo è praticamente quello che volevo fare in uno degli ultimi progetti che ho svolto in università.]]></summary></entry></feed>